1. Чтобы при обновлении всегда было минимальное количество pod такое же как в `spec.replicas` нужно выставить: `spec.strategy.rollingUpdate.maxUnavailable: 0`
2. Настройте [Readiness и Startup](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/) пробы чтобы избежать ошибок таких как отправка трафика на pod приложение в котором еще не запущенно (Liveness пробу нужно ставить только в том случае если лучше что можно сделать это убить контейнер)
3. Для того чтобы k8s успел убрать pod из endpoint для service перед отправкой SIGTERM сигнала необходимо добавить sleep перед остановкой (который значительно меньше `terminationGracePeriodSeconds`) чтобы избежать ошибки в случаях:
   - когда HPA уменьшает количество pod 
   - удалении старых pod при обновлении

   Для версии ниже 1.29
    ```yaml
    spec:
      template:
        spec:
          terminationGracePeriodSeconds: 45
          containers:
            - name: myapp
              lifecycle:
                preStop:
                  exec:
                    command:
                      - /bin/sh
                      - '-c'
                      - /bin/sleep 15
    ```
    Для версии 1.29+
    ```yaml
    spec:
      template:
        spec:
          terminationGracePeriodSeconds: 35
          containers:
            - name: myapp
              lifecycle:
                preStop:
                  sleep:
                    seconds: 5
    ```
5. Не забывайте выставлять limits и requests для ваших контейнеров в pod чтобы k8s использовал requests для планирования, а limits для ограничения конкуренции за ресурсов на node (простой способ как подобрать написано выше в Deployment) 
6. Настройте HPA для вашего Deployment с запасом 2х от текущей нагрузки (по результатам нагрузочного тестирования)
   > **Например:** с максимальной дневной нагрузкой за 7 дней справляются 6 pod, выставляем `maxReplicas: 12` (смотрим чтобы не было блокеров, например чтобы не закончились подключения к БД и тд)

### Service mesh настройки

6. Настройте в вашем service mesh политику повторных запросов при ошибке от нижестоящего сервиса:
   - в istio по умолчанию 2 повторные попытки для virtialService который используется как ingress https://istio.io/latest/docs/concepts/traffic-management/#retries
   - в linkerd можно включить опционально https://linkerd.io/2.16/tasks/configuring-retries/
8. Если ваше приложение зависит от внешних служб, вам следует проверить, доступны ли эти службы, прежде чем разрешать Kubernetes направлять трафик к экземпляру приложения. Имейте в виду, что прокси-сервер service mesh sidecar может запускаться медленнее, чем ваше приложение. Это означает, что при запуске приложения вы должны повторить попытку хотя бы в течение пары секунд перед любым внешним подключением. Как избежать этого в различных service mesh:
   - Istio: `holdApplicationUntilProxyStarts=true` https://istio.io/latest/docs/ops/common-problems/injection/#pod-or-containers-start-with-network-issues-if-istio-proxy-is-not-ready
   - Linkerd: `config.linkerd.io/proxy-await=enabled` https://linkerd.io/2.16/reference/proxy-configuration/
